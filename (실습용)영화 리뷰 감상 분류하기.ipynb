{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfIJuQplvNs89NRzylEln/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dudgus7477/datascience/blob/main/(%EC%8B%A4%EC%8A%B5%EC%9A%A9)%EC%98%81%ED%99%94%20%EB%A6%AC%EB%B7%B0%20%EA%B0%90%EC%83%81%20%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영화리뷰에 대한 텍스트와 해당 리뷰가 긍정인 경우 1, 부정인 경우 0으로 표시(label)\n",
        "- 감성 분류를 수행하는 모델을 만들어보자!"
      ],
      "metadata": {
        "id": "qOyNronMYcLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 필요한 라이브러리, 데이터 가져오기"
      ],
      "metadata": {
        "id": "XKMi7OMiaSs2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMCoVr4GX7ZP",
        "outputId": "81a11edd-46a5-4a39-ef80-3ad9b325876b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_table('./ratings_test.txt')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fd-I8Uu8ZmXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터에 존재하는 영화리뷰 개수를 확인해봅시다."
      ],
      "metadata": {
        "id": "cSwAti04aECy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰 개수 :',len(df))"
      ],
      "metadata": {
        "id": "j0OHcOXsaA7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. 데이터 전처리\n",
        "- 중복 유무 확인 및 제거"
      ],
      "metadata": {
        "id": "bnGDp1v6aYol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# document 열의 중복을 제외한 값의 개수\n",
        "\n",
        "# document 열의 중복 제거"
      ],
      "metadata": {
        "id": "6HmqCEbcak-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#빈 곳의 개수\n",
        "\n",
        "#빈 부분 제거\n"
      ],
      "metadata": {
        "id": "v8Z7zS_ma7ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#총 데이터의 개수\n",
        "print('총 샘플의 수 :', len(df))"
      ],
      "metadata": {
        "id": "frRtnrZ3bYxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 해당 리뷰의 긍,부정 유무가 기재되어있는 레이블(lable)값의 분포를 확인해보자!"
      ],
      "metadata": {
        "id": "CaX-vVhTbtkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts().plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "DAKhj_EKb4aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글과 공백을 제외하고 모두 제거해보자!(ex. 특수문자)"
      ],
      "metadata": {
        "id": "2_s_SjlzcVbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글과 공백을 제외하고 모두 제거"
      ],
      "metadata": {
        "id": "hMS14eOBciVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#빈 곳의 개수\n",
        "\n",
        "#빈 부분 제거\n"
      ],
      "metadata": {
        "id": "G-8GWWQ4c9LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. 형태소로 토큰화하기\n",
        "- 저번 시간에 배운 내용 복습해보자!"
      ],
      "metadata": {
        "id": "l5MV_Cn2dlzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#리뷰 형태소로 토큰화하기\n"
      ],
      "metadata": {
        "id": "mp9ws5itdsA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUI18ZgIzmTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xG9_dlXNGreJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 기계가 텍스트를 숫자로 처리할 수 있도록 정수 인코딩하기!"
      ],
      "metadata": {
        "id": "jml7uJpY0-87"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4p3muHp09Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ud7yDbqY1rRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어가 26,000개가 넘게 존재!\n",
        "- 여기서 각 정수는 빈도수가 높은 순서대로 부여됨(즉 높은 정수가 빈도수가 낮음)\n",
        "> 빈도수가 2이하인 희귀단어를 제외시켜보자!"
      ],
      "metadata": {
        "id": "GKGQVKTK24bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#빈도수 체크\n",
        "hold = 3\n",
        "total = len(tokenizer.word_index) # 단어의 수\n",
        "rare = 0   # 등장 빈도수가 hold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0   # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0   # 등장 빈도수가 hold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "\n",
        "\n",
        "    # 단어의 등장 빈도수가 hold보다 작으면\n",
        "\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(hold - 1, rare))\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"
      ],
      "metadata": {
        "id": "bC4ufRJt3lMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 등장 빈도가 2회 이하인 단어들은 제외"
      ],
      "metadata": {
        "id": "JmQI-uqB5pBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 시퀀스 자료형이란?\n",
        " 리스트와 같이 각각의 요소들이 연속적으로 이어진 자료형\n",
        "\n"
      ],
      "metadata": {
        "id": "TVJbbt796Z4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 시퀀스로 변환(리스트 형식으로 저장하기위해)"
      ],
      "metadata": {
        "id": "yFEGDaEt6JM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. 패딩\n",
        "- 서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 작업"
      ],
      "metadata": {
        "id": "6UlqLyvwLS-a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpeSVQogLbd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEREZQmlLwTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 모델 만들기"
      ],
      "metadata": {
        "id": "WVaGShLh-oVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM #다대일 구조의 LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='          '))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) #손실함수는 크로스 엔트로피 함수\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "aPmNQDFDPwKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정확도 확인\n"
      ],
      "metadata": {
        "id": "XrmYAlLUJA5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. 리뷰 예측하기"
      ],
      "metadata": {
        "id": "MWAXPqaXOYLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = 30) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "\n",
        "#긍정, 부정 리뷰 구분\n"
      ],
      "metadata": {
        "id": "tx4eYzqzOasX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ak_WdOGYOg74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1FIXVW8Oj3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}